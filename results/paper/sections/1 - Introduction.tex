
Gzip~\cite{RFC1952} and \deflate~\cite{RFC1951} are compression file formats that are ubiquitously used to compress single files and file agglomerates like \tar archives, databases, and network transmissions.
A gzip file contains one or more gzip streams each containing a Deflate stream and additional metadata.
Each \deflate stream contains one or more \deflateblocks.
Three kinds of \deflateblocks exist: \rawblocks, \dynblocks, and \fixedblocks.
\dynblocks, and \fixedblocks are compressed using a combination of Huffman coding~\cite{huffman} and a variation of the Lempel–Ziv–Storer–Szymanski (LZSS) algorithm~\cite{lzss}.
LZSS is a variation of LZ77~\cite{lz77}, which compresses substrings with backward pointers to the preceding stream.

Deflate streams can also be found in other file formats like ZIP~\cite{zip}, PNG~\cite{png} and other ZIP-based file formats like JAR~\cite{jar}, XLSX~\cite{xlsx} and ODT~\cite{odt}.


\subsection{Motivation}

Gzip-compressed TAR files can grow to several terabytes, e.g., the updated \SI{1.19}{\tera\byte} large gzip-compressed ImageNet archive~\cite{imagenet21k}.
The multiple petabytes large Common Crawl~\cite{commoncrawl} dataset is also distributed as a set of gzip-compressed files.
A pipeline for decompressing and preprocessing such data, e.g., for use in machine learning context, would benefit from being accelerated by multi-threaded decompression.
Extracting a \SI{1}{\tera\byte} large file with \gzip takes several hours.
Assuming a sufficiently fast file system and two 64-core processors, this can be reduced by a factor of \num{55} to several minutes when using \pragzipname.

Gzip files are only slowly getting replaced by newer compression formats like Zstandard~\cite{zstandard} because of the wide-spread availability of ZLIB~\cite{zlib} and \gzip for compressing and decompressing gzip streams.
Parallel decompression of gzip becomes desirable when working with gzip-compressed data from external sources because the compression format cannot be chosen.
If the compression format can be chosen and if decompression support is not an issue, then newer formats like Zstandard are more efficient.
However, as we show in \cref{sct:compression-formats}, decompression with \pragzipname using 128 cores can be twice as fast as the parallel Zstandard decompression tool \pzstd given the same amount of cores.



\subsection{Limitation of State-of-Art Approaches}
\label{sct:limitations}


Although a multitude of implementations for gzip exist, none of them can fully utilize current multi-core processor architectures for decompressing gzip-compressed files without restrictions on the contents.
The Blocked GNU Zip Format (BGZF)~\cite{htslib} is a subset of gzip files that adds the encoded size of the gzip stream into the metadata of the gzip stream header.
With this metadata, another thread can seek to the next gzip stream and start decoding it in parallel.
The command line tool \CLITOOL{bgzip} uses multi-threading to decompress BGZF files.
It cannot parallelize the decompression of gzip files that do not contain the metadata defined by BGZF.

\citeauthor{pugz}~\cite{pugz} show that gzip decompression can be split into two stages, each of which can be parallelized.
For parallelization, the input data is divided into chunks, which are sent to decompression threads.
In the first stage, each thread searches for the first \deflateblock in its assigned chunk and starts first-stage decompression from there.
Searching for \deflateblocks may result in false positives, i.e., positions in the stream that are indistinguishable from a valid \deflateblock without parsing the whole preceding compressed stream.
The output of the first-stage decompression may contain markers for data referenced by backward pointers that cannot be resolved without knowing the preceding decompressed data.
These markers are resolved in the second stage.

There are some limitations to this approach:
\begin{itemize}
    \item Parallelization works on a \deflateblock granularity.
          Gzip streams containing fewer \deflateblocks than the degree of parallelization aimed for cannot be effectively parallelized.
    \item False positives for \deflateblocks cannot be prevented without knowing the preceding \deflate stream.
    \item There is overhead for the two stages, the intermediate format, which is twice the size of the result, and for finding new blocks.
          The larger the overhead for finding a block is, the larger the chunk size has to be chosen to achieve a speedup close to ideal linear scaling.
          This imposes a lower bound on the chunk size for reaching optimal computational performance.
    \item The memory usage is proportional to the degree of parallelism, the chunk size, and the compression ratio because the decompressed data of each chunk has to be stored in memory.
\end{itemize}

An implementation of this algorithm exists in the command line tool \pugz.
To reduce false positives when searching for \deflateblocks, it requires that the decompressed data stream only contains bytes with values 9--126 and decompresses to at least \SI{1}{\kibi\byte} and up to \SI{4}{\mebi\byte}.
It is not able to decompress arbitrary gzip files.

Furthermore, chunks are distributed to the parallel threads in a fixed uniform manner and therefore can lead to workload imbalances caused by varying compression factors in each chunk.
Because it is based on libdeflate~\cite{libdeflate}, it also has the technical limitation that the output buffer size for each chunk has to be configured before decompression.
The output buffer is set to \SI{512}{\mebi\byte} of decompressed data for each of the \SI{32}{\mebi\byte} chunks in the compressed data stream, meaning it will fail for files with a compression ratio larger than \num{16}.

\subsection{Key Insights and Contributions}


Our implementation, \pragzip\anon[\footnote{Name has been changed for double blind review}]{}, addresses the limitations of \pugz by implementing a cache and a parallelized prefetcher.
It is open-source software and available on Github\footnote{\anon[URL removed for double-blind review]{\url{https://github.com/mxmlnkn/indexed_bzip2}}}.
Our architecture solves the following issues:
\begin{itemize}
    \item False positives that are found when searching for \deflate blocks are inserted into the cache but are never used and will be evicted.
          This makes the architecture robust against false positives and enables us to generalize gzip decompression to arbitrary gzip files by removing the non-generalizing \deflate block finder checks.
    \item The prefetched chunks are pushed into a thread pool for workload balanced parallel decompression.
    \item Non-sequential access patterns are supported efficiently.
    \item Gzip files with more than one gzip stream are supported.
    \item Abstraction of file reading enables support for Python file-like objects.
\end{itemize}

Caching and prefetching are techniques commonly used in processors to optimize main memory access~\cite{smith1982cache}, file access~\cite{mpi-io-caching}, and other domains.
We have successfully applied these techniques for chunks of decompressed data.
This enables us to provide not only parallelized decompression but also constant-time random access into the decompressed stream given an index containing seek points.
While a proof-of-concept for parallel decompression for specialized gzip-compressed data was available with \pugz~\cite{pugz} and random access into decompressed gzip streams was available with \texttt{indexed\_gzip}~\cite{indexed_gzip}, we have successfully combined these two capabilities and made them usable for arbitrary gzip files.

\paragraph{Index for Seeking}
An index containing seek points is built during decompression.
Each seek point contains the compressed bit offset, the decompressed byte offset, and the last \SI{32}{\kibi\byte} of decompressed data from the previous block.
Decompression can be resumed at each seek point without decompressing anything before it.
The range between the previous seek point and the requested offset is decompressed in order to reach offsets between seek points.
The overhead for this is bounded because the seek point spacing can be configured to a maximum value.

Our implementation's seeking and decompression capabilities can also be used via a library interface to provide a light-weight layer to access the compressed file contents as done by ratar\-mount \cite{ratarmount}.
The seek point index can be exported and imported similarly to \texttt{indexed\_gzip}~\cite{indexed_gzip} to avoid the decompression time for the initial decompression pass.
Furthermore, decompression can be delegated to an optimized gzip implementation like zlib~\cite{zlib} when the index has been loaded.
This is more than twice as fast as the two-stage decompression.
Lastly, loading the index also improves load balancing and reduces memory usage because the seek points are equally spaced in the decompressed stream.

\paragraph{Performance Analysis}

We also provide benchmark results for all components of \pragzipname in \cref{sct:evaluation} in order to give a performance overview and determine bottlenecks.
We improve one such bottleneck, the searching for \deflate blocks, by implementing a skip table.
This block finder is 4 times faster than the one implemented in \pugz.
A faster block finder makes it possible to reduce the chunk size and, therefore, the memory requirements accordingly while retaining the same overall performance.
The fast \deflate \blockfinder also improves the speed for the recovery of corrupted gzip files.


\subsection{Limitations of the Proposed Approach}


The main limiting factor of \pragzipname is memory usage.
Each chunk of decompressed data currently being prefetched and contained in the cache is held in memory.
The prefetch cache holds twice as many chunks as the degree of parallelization.
The compressed chunk size can be configured and is \SI{4}{\mebi\byte} by default.
During index creation, large chunks are split when necessary to ensure that the maximum decompressed chunk size is not larger than the configured chunk size.
Therefore, the maximum memory requirement is only \SI{8}{\mebi\byte} per thread if an existing index is used for decompression.
The memory required for each chunk depends on the compression ratio of the file when decompressing without an existing index.
Compression factors are often below \num{10}, which translates to a memory requirement of \SI{80}{\mebi\byte} per thread.
In the worst case, for a file with the largest possible compression factor of \num{1032}, the memory requirement for decompression would be \SI{8.3}{\gibi\byte} per thread.
This can be mitigated by implementing a fallback to sequential decompression for chunks with large compression ratios or by splitting the chunk sizes dynamically.

A second limitation is that the achievable parallelization is limited by the number of \rawblocks and \dynblocks in the file.
If the file consists of a single gzip stream with a single \deflateblock, then \pragzipname cannot parallelize decompression.
This is a limitation of the two-stage decompression scheme.
In contrast to \pugz, the block finder in \pragzipname does not look for \deflate blocks with Fixed Huffman Codes.
If a file consists only of such \deflate blocks, then the decompression will not be parallelized with our implementation.
With default compression settings, such blocks occur only rarely, namely for very small files and at the end of a gzip stream.
In \cref{sct:gzip-compressors}, we show that most gzip compression tools and compression level settings result in gzip files that can be decompressed in parallel with \pragzipname.
