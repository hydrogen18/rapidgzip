
We have developed an architecture for parallel decompression and seeking in gzip files based on a cache-and-prefetch architecture.
This architecture is robust against false positives returned by the block finder.
This robustness enables us to extend the parallel gzip decompression algorithm presented by \citeauthor{pugz}~\cite{pugz}, which was limited to files containing bytes in the range 9--126, to all kinds of gzip-compressed files.
From this new implementation also emerge other improvements such as load balancing and support for multi-part gzip files.
The dynamic work distribution also improves upon the observed slowdowns of \pugz when writing the result to a file in the correct order.

We have implemented this architecture in the command line tool and library \pragzip.
We have achieved decompression bandwidths of \SI{8.7}{\giga\byte/\second} for base64-encoded random data,
  and \SI{5.6}{\giga\byte/\second} for the Silesia dataset when using 128 cores for parallelization.
This is $\num{21}\times$ and $\num{8.5}\times$ faster, respectively, than \CLITOOL{igzip}~\cite{igzip}, the fastest single-threaded gzip decompression tool known to us.
When compared to GNU gzip version 1.12, it is $\num{55}\times$ and $\num{33}\times$ faster, respectively.

In the future, we intend to add checksum computation and further optimizations to the custom \deflate implementation.
We will also address the limitations mentioned in~\cref{sct:limitations}.
In particular, the work splitting of chunks with very high decompression ratios will reduce the maximum memory requirements and further improve load balancing.
